{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv('wine.data.txt',names=['Class','Alcohol','MalicAcid','Ash','AlcofAsh','Mg','Totphen','Flv','NFP','Proan','ColInt',\n",
    "                                          'Hue','OD--Dilwine','Proline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>MalicAcid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>AlcofAsh</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Totphen</th>\n",
       "      <th>Flv</th>\n",
       "      <th>NFP</th>\n",
       "      <th>Proan</th>\n",
       "      <th>ColInt</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD--Dilwine</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class  Alcohol  MalicAcid   Ash  AlcofAsh   Mg  Totphen   Flv   NFP  Proan  \\\n",
       "0      1    14.23       1.71  2.43      15.6  127     2.80  3.06  0.28   2.29   \n",
       "1      1    13.20       1.78  2.14      11.2  100     2.65  2.76  0.26   1.28   \n",
       "2      1    13.16       2.36  2.67      18.6  101     2.80  3.24  0.30   2.81   \n",
       "3      1    14.37       1.95  2.50      16.8  113     3.85  3.49  0.24   2.18   \n",
       "4      1    13.24       2.59  2.87      21.0  118     2.80  2.69  0.39   1.82   \n",
       "\n",
       "   ColInt   Hue  OD--Dilwine  Proline  \n",
       "0    5.64  1.04         3.92     1065  \n",
       "1    4.38  1.05         3.40     1050  \n",
       "2    5.68  1.03         3.17     1185  \n",
       "3    7.80  0.86         3.45     1480  \n",
       "4    4.32  1.04         2.93      735  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <td>178.0</td>\n",
       "      <td>1.938202</td>\n",
       "      <td>0.775035</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alcohol</th>\n",
       "      <td>178.0</td>\n",
       "      <td>13.000618</td>\n",
       "      <td>0.811827</td>\n",
       "      <td>11.03</td>\n",
       "      <td>12.3625</td>\n",
       "      <td>13.050</td>\n",
       "      <td>13.6775</td>\n",
       "      <td>14.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MalicAcid</th>\n",
       "      <td>178.0</td>\n",
       "      <td>2.336348</td>\n",
       "      <td>1.117146</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.6025</td>\n",
       "      <td>1.865</td>\n",
       "      <td>3.0825</td>\n",
       "      <td>5.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ash</th>\n",
       "      <td>178.0</td>\n",
       "      <td>2.366517</td>\n",
       "      <td>0.274344</td>\n",
       "      <td>1.36</td>\n",
       "      <td>2.2100</td>\n",
       "      <td>2.360</td>\n",
       "      <td>2.5575</td>\n",
       "      <td>3.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AlcofAsh</th>\n",
       "      <td>178.0</td>\n",
       "      <td>19.494944</td>\n",
       "      <td>3.339564</td>\n",
       "      <td>10.60</td>\n",
       "      <td>17.2000</td>\n",
       "      <td>19.500</td>\n",
       "      <td>21.5000</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mg</th>\n",
       "      <td>178.0</td>\n",
       "      <td>99.741573</td>\n",
       "      <td>14.282484</td>\n",
       "      <td>70.00</td>\n",
       "      <td>88.0000</td>\n",
       "      <td>98.000</td>\n",
       "      <td>107.0000</td>\n",
       "      <td>162.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Totphen</th>\n",
       "      <td>178.0</td>\n",
       "      <td>2.295112</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.7425</td>\n",
       "      <td>2.355</td>\n",
       "      <td>2.8000</td>\n",
       "      <td>3.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flv</th>\n",
       "      <td>178.0</td>\n",
       "      <td>2.029270</td>\n",
       "      <td>0.998859</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.2050</td>\n",
       "      <td>2.135</td>\n",
       "      <td>2.8750</td>\n",
       "      <td>5.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NFP</th>\n",
       "      <td>178.0</td>\n",
       "      <td>0.361854</td>\n",
       "      <td>0.124453</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.2700</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Proan</th>\n",
       "      <td>178.0</td>\n",
       "      <td>1.590899</td>\n",
       "      <td>0.572359</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.2500</td>\n",
       "      <td>1.555</td>\n",
       "      <td>1.9500</td>\n",
       "      <td>3.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ColInt</th>\n",
       "      <td>178.0</td>\n",
       "      <td>5.058090</td>\n",
       "      <td>2.318286</td>\n",
       "      <td>1.28</td>\n",
       "      <td>3.2200</td>\n",
       "      <td>4.690</td>\n",
       "      <td>6.2000</td>\n",
       "      <td>13.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hue</th>\n",
       "      <td>178.0</td>\n",
       "      <td>0.957449</td>\n",
       "      <td>0.228572</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.7825</td>\n",
       "      <td>0.965</td>\n",
       "      <td>1.1200</td>\n",
       "      <td>1.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OD--Dilwine</th>\n",
       "      <td>178.0</td>\n",
       "      <td>2.611685</td>\n",
       "      <td>0.709990</td>\n",
       "      <td>1.27</td>\n",
       "      <td>1.9375</td>\n",
       "      <td>2.780</td>\n",
       "      <td>3.1700</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Proline</th>\n",
       "      <td>178.0</td>\n",
       "      <td>746.893258</td>\n",
       "      <td>314.907474</td>\n",
       "      <td>278.00</td>\n",
       "      <td>500.5000</td>\n",
       "      <td>673.500</td>\n",
       "      <td>985.0000</td>\n",
       "      <td>1680.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count        mean         std     min       25%      50%  \\\n",
       "Class        178.0    1.938202    0.775035    1.00    1.0000    2.000   \n",
       "Alcohol      178.0   13.000618    0.811827   11.03   12.3625   13.050   \n",
       "MalicAcid    178.0    2.336348    1.117146    0.74    1.6025    1.865   \n",
       "Ash          178.0    2.366517    0.274344    1.36    2.2100    2.360   \n",
       "AlcofAsh     178.0   19.494944    3.339564   10.60   17.2000   19.500   \n",
       "Mg           178.0   99.741573   14.282484   70.00   88.0000   98.000   \n",
       "Totphen      178.0    2.295112    0.625851    0.98    1.7425    2.355   \n",
       "Flv          178.0    2.029270    0.998859    0.34    1.2050    2.135   \n",
       "NFP          178.0    0.361854    0.124453    0.13    0.2700    0.340   \n",
       "Proan        178.0    1.590899    0.572359    0.41    1.2500    1.555   \n",
       "ColInt       178.0    5.058090    2.318286    1.28    3.2200    4.690   \n",
       "Hue          178.0    0.957449    0.228572    0.48    0.7825    0.965   \n",
       "OD--Dilwine  178.0    2.611685    0.709990    1.27    1.9375    2.780   \n",
       "Proline      178.0  746.893258  314.907474  278.00  500.5000  673.500   \n",
       "\n",
       "                  75%      max  \n",
       "Class          3.0000     3.00  \n",
       "Alcohol       13.6775    14.83  \n",
       "MalicAcid      3.0825     5.80  \n",
       "Ash            2.5575     3.23  \n",
       "AlcofAsh      21.5000    30.00  \n",
       "Mg           107.0000   162.00  \n",
       "Totphen        2.8000     3.88  \n",
       "Flv            2.8750     5.08  \n",
       "NFP            0.4375     0.66  \n",
       "Proan          1.9500     3.58  \n",
       "ColInt         6.2000    13.00  \n",
       "Hue            1.1200     1.71  \n",
       "OD--Dilwine    3.1700     4.00  \n",
       "Proline      985.0000  1680.00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 178 entries, 0 to 177\n",
      "Data columns (total 14 columns):\n",
      "Class          178 non-null int64\n",
      "Alcohol        178 non-null float64\n",
      "MalicAcid      178 non-null float64\n",
      "Ash            178 non-null float64\n",
      "AlcofAsh       178 non-null float64\n",
      "Mg             178 non-null int64\n",
      "Totphen        178 non-null float64\n",
      "Flv            178 non-null float64\n",
      "NFP            178 non-null float64\n",
      "Proan          178 non-null float64\n",
      "ColInt         178 non-null float64\n",
      "Hue            178 non-null float64\n",
      "OD--Dilwine    178 non-null float64\n",
      "Proline        178 non-null int64\n",
      "dtypes: float64(11), int64(3)\n",
      "memory usage: 19.5 KB\n"
     ]
    }
   ],
   "source": [
    "wine.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    71\n",
       "1    59\n",
       "3    48\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.Class.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective -  Dimensionality Reduction through Principal Component Analysis on the Wine data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimentionality reduction can be done directly by using PCA function, but in this project we will demonstrate the step by step approach of how PCA reaches its final principal components and also describe the parameter to choose number pf principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Step by Step approach to get the principal components are given below with respect to the wine dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\tStandardize the d-dimensional dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into dependent and independent variables\n",
    "\n",
    "x= wine.drop('Class', axis=1)\n",
    "y= wine.Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into test and train\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.3, random_state=1)\n",
    "\n",
    "#Standardizing the x_train and x_test\n",
    "sc = StandardScaler()\n",
    "x_train_std = sc.fit_transform(x_train)\n",
    "x_test_std = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\tConstruct the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance Matrix:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.008130</td>\n",
       "      <td>0.096953</td>\n",
       "      <td>0.227389</td>\n",
       "      <td>-0.275084</td>\n",
       "      <td>0.269558</td>\n",
       "      <td>0.235365</td>\n",
       "      <td>0.176355</td>\n",
       "      <td>-0.106197</td>\n",
       "      <td>0.083915</td>\n",
       "      <td>0.526589</td>\n",
       "      <td>-0.127941</td>\n",
       "      <td>0.038045</td>\n",
       "      <td>0.587832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.096953</td>\n",
       "      <td>1.008130</td>\n",
       "      <td>0.149770</td>\n",
       "      <td>0.314273</td>\n",
       "      <td>0.009054</td>\n",
       "      <td>-0.327302</td>\n",
       "      <td>-0.400049</td>\n",
       "      <td>0.251178</td>\n",
       "      <td>-0.142756</td>\n",
       "      <td>0.240464</td>\n",
       "      <td>-0.609794</td>\n",
       "      <td>-0.373695</td>\n",
       "      <td>-0.187304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.227389</td>\n",
       "      <td>0.149770</td>\n",
       "      <td>1.008130</td>\n",
       "      <td>0.458301</td>\n",
       "      <td>0.389166</td>\n",
       "      <td>0.106299</td>\n",
       "      <td>0.078435</td>\n",
       "      <td>0.170165</td>\n",
       "      <td>0.011716</td>\n",
       "      <td>0.228964</td>\n",
       "      <td>-0.074566</td>\n",
       "      <td>0.007068</td>\n",
       "      <td>0.238553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.275084</td>\n",
       "      <td>0.314273</td>\n",
       "      <td>0.458301</td>\n",
       "      <td>1.008130</td>\n",
       "      <td>-0.037761</td>\n",
       "      <td>-0.305006</td>\n",
       "      <td>-0.347292</td>\n",
       "      <td>0.294066</td>\n",
       "      <td>-0.160245</td>\n",
       "      <td>0.039068</td>\n",
       "      <td>-0.281778</td>\n",
       "      <td>-0.265257</td>\n",
       "      <td>-0.436090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.269558</td>\n",
       "      <td>0.009054</td>\n",
       "      <td>0.389166</td>\n",
       "      <td>-0.037761</td>\n",
       "      <td>1.008130</td>\n",
       "      <td>0.157458</td>\n",
       "      <td>0.139934</td>\n",
       "      <td>-0.162506</td>\n",
       "      <td>0.174812</td>\n",
       "      <td>0.282324</td>\n",
       "      <td>-0.053770</td>\n",
       "      <td>-0.012655</td>\n",
       "      <td>0.418530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.235365</td>\n",
       "      <td>-0.327302</td>\n",
       "      <td>0.106299</td>\n",
       "      <td>-0.305006</td>\n",
       "      <td>0.157458</td>\n",
       "      <td>1.008130</td>\n",
       "      <td>0.874138</td>\n",
       "      <td>-0.424767</td>\n",
       "      <td>0.640804</td>\n",
       "      <td>-0.129229</td>\n",
       "      <td>0.475100</td>\n",
       "      <td>0.723500</td>\n",
       "      <td>0.439876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.176355</td>\n",
       "      <td>-0.400049</td>\n",
       "      <td>0.078435</td>\n",
       "      <td>-0.347292</td>\n",
       "      <td>0.139934</td>\n",
       "      <td>0.874138</td>\n",
       "      <td>1.008130</td>\n",
       "      <td>-0.544010</td>\n",
       "      <td>0.659196</td>\n",
       "      <td>-0.230606</td>\n",
       "      <td>0.579895</td>\n",
       "      <td>0.794950</td>\n",
       "      <td>0.415790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.106197</td>\n",
       "      <td>0.251178</td>\n",
       "      <td>0.170165</td>\n",
       "      <td>0.294066</td>\n",
       "      <td>-0.162506</td>\n",
       "      <td>-0.424767</td>\n",
       "      <td>-0.544010</td>\n",
       "      <td>1.008130</td>\n",
       "      <td>-0.334451</td>\n",
       "      <td>0.190302</td>\n",
       "      <td>-0.274434</td>\n",
       "      <td>-0.540646</td>\n",
       "      <td>-0.264144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.083915</td>\n",
       "      <td>-0.142756</td>\n",
       "      <td>0.011716</td>\n",
       "      <td>-0.160245</td>\n",
       "      <td>0.174812</td>\n",
       "      <td>0.640804</td>\n",
       "      <td>0.659196</td>\n",
       "      <td>-0.334451</td>\n",
       "      <td>1.008130</td>\n",
       "      <td>-0.053892</td>\n",
       "      <td>0.293701</td>\n",
       "      <td>0.503793</td>\n",
       "      <td>0.285637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.526589</td>\n",
       "      <td>0.240464</td>\n",
       "      <td>0.228964</td>\n",
       "      <td>0.039068</td>\n",
       "      <td>0.282324</td>\n",
       "      <td>-0.129229</td>\n",
       "      <td>-0.230606</td>\n",
       "      <td>0.190302</td>\n",
       "      <td>-0.053892</td>\n",
       "      <td>1.008130</td>\n",
       "      <td>-0.546602</td>\n",
       "      <td>-0.455576</td>\n",
       "      <td>0.288943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.127941</td>\n",
       "      <td>-0.609794</td>\n",
       "      <td>-0.074566</td>\n",
       "      <td>-0.281778</td>\n",
       "      <td>-0.053770</td>\n",
       "      <td>0.475100</td>\n",
       "      <td>0.579895</td>\n",
       "      <td>-0.274434</td>\n",
       "      <td>0.293701</td>\n",
       "      <td>-0.546602</td>\n",
       "      <td>1.008130</td>\n",
       "      <td>0.577103</td>\n",
       "      <td>0.137498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.038045</td>\n",
       "      <td>-0.373695</td>\n",
       "      <td>0.007068</td>\n",
       "      <td>-0.265257</td>\n",
       "      <td>-0.012655</td>\n",
       "      <td>0.723500</td>\n",
       "      <td>0.794950</td>\n",
       "      <td>-0.540646</td>\n",
       "      <td>0.503793</td>\n",
       "      <td>-0.455576</td>\n",
       "      <td>0.577103</td>\n",
       "      <td>1.008130</td>\n",
       "      <td>0.289115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.587832</td>\n",
       "      <td>-0.187304</td>\n",
       "      <td>0.238553</td>\n",
       "      <td>-0.436090</td>\n",
       "      <td>0.418530</td>\n",
       "      <td>0.439876</td>\n",
       "      <td>0.415790</td>\n",
       "      <td>-0.264144</td>\n",
       "      <td>0.285637</td>\n",
       "      <td>0.288943</td>\n",
       "      <td>0.137498</td>\n",
       "      <td>0.289115</td>\n",
       "      <td>1.008130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   1.008130  0.096953  0.227389 -0.275084  0.269558  0.235365  0.176355   \n",
       "1   0.096953  1.008130  0.149770  0.314273  0.009054 -0.327302 -0.400049   \n",
       "2   0.227389  0.149770  1.008130  0.458301  0.389166  0.106299  0.078435   \n",
       "3  -0.275084  0.314273  0.458301  1.008130 -0.037761 -0.305006 -0.347292   \n",
       "4   0.269558  0.009054  0.389166 -0.037761  1.008130  0.157458  0.139934   \n",
       "5   0.235365 -0.327302  0.106299 -0.305006  0.157458  1.008130  0.874138   \n",
       "6   0.176355 -0.400049  0.078435 -0.347292  0.139934  0.874138  1.008130   \n",
       "7  -0.106197  0.251178  0.170165  0.294066 -0.162506 -0.424767 -0.544010   \n",
       "8   0.083915 -0.142756  0.011716 -0.160245  0.174812  0.640804  0.659196   \n",
       "9   0.526589  0.240464  0.228964  0.039068  0.282324 -0.129229 -0.230606   \n",
       "10 -0.127941 -0.609794 -0.074566 -0.281778 -0.053770  0.475100  0.579895   \n",
       "11  0.038045 -0.373695  0.007068 -0.265257 -0.012655  0.723500  0.794950   \n",
       "12  0.587832 -0.187304  0.238553 -0.436090  0.418530  0.439876  0.415790   \n",
       "\n",
       "          7         8         9         10        11        12  \n",
       "0  -0.106197  0.083915  0.526589 -0.127941  0.038045  0.587832  \n",
       "1   0.251178 -0.142756  0.240464 -0.609794 -0.373695 -0.187304  \n",
       "2   0.170165  0.011716  0.228964 -0.074566  0.007068  0.238553  \n",
       "3   0.294066 -0.160245  0.039068 -0.281778 -0.265257 -0.436090  \n",
       "4  -0.162506  0.174812  0.282324 -0.053770 -0.012655  0.418530  \n",
       "5  -0.424767  0.640804 -0.129229  0.475100  0.723500  0.439876  \n",
       "6  -0.544010  0.659196 -0.230606  0.579895  0.794950  0.415790  \n",
       "7   1.008130 -0.334451  0.190302 -0.274434 -0.540646 -0.264144  \n",
       "8  -0.334451  1.008130 -0.053892  0.293701  0.503793  0.285637  \n",
       "9   0.190302 -0.053892  1.008130 -0.546602 -0.455576  0.288943  \n",
       "10 -0.274434  0.293701 -0.546602  1.008130  0.577103  0.137498  \n",
       "11 -0.540646  0.503793 -0.455576  0.577103  1.008130  0.289115  \n",
       "12 -0.264144  0.285637  0.288943  0.137498  0.289115  1.008130  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We will make covariance matrix using multiple columns which will make 13*13 matrix\n",
    "\n",
    "cov_mat = np.cov(x_train_std.T)\n",
    "print('Covariance Matrix:\\n')\n",
    "pd.DataFrame(cov_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\tDecompose the covariance matrix into its eigenvectors and eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Eigen values: \n",
      " [4.60904624 2.58694581 1.48342648 1.01296796 0.80274502 0.67220992\n",
      " 0.55435313 0.10226076 0.3440145  0.3179616  0.17356328 0.23908289\n",
      " 0.20711346]\n"
     ]
    }
   ],
   "source": [
    "#Calculation of eigen value and eigen vector\n",
    "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "#Print Eigen Values\n",
    "print('\\nEigen values: \\n', eigen_vals)\n",
    "\n",
    "#Eigen values represent the magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Eigen Vector\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.090411</td>\n",
       "      <td>-0.476735</td>\n",
       "      <td>0.232385</td>\n",
       "      <td>-0.006533</td>\n",
       "      <td>-0.294327</td>\n",
       "      <td>-0.343637</td>\n",
       "      <td>0.065967</td>\n",
       "      <td>0.028118</td>\n",
       "      <td>-0.548782</td>\n",
       "      <td>-0.164379</td>\n",
       "      <td>-0.179305</td>\n",
       "      <td>-0.358245</td>\n",
       "      <td>-0.109431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.252988</td>\n",
       "      <td>-0.176995</td>\n",
       "      <td>-0.140683</td>\n",
       "      <td>-0.591582</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>-0.252437</td>\n",
       "      <td>-0.546378</td>\n",
       "      <td>0.002873</td>\n",
       "      <td>-0.127716</td>\n",
       "      <td>-0.033814</td>\n",
       "      <td>0.175765</td>\n",
       "      <td>0.296933</td>\n",
       "      <td>0.190071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.016027</td>\n",
       "      <td>-0.323900</td>\n",
       "      <td>-0.589064</td>\n",
       "      <td>0.296520</td>\n",
       "      <td>-0.119851</td>\n",
       "      <td>-0.229368</td>\n",
       "      <td>-0.016998</td>\n",
       "      <td>-0.147944</td>\n",
       "      <td>0.141526</td>\n",
       "      <td>0.019954</td>\n",
       "      <td>-0.092880</td>\n",
       "      <td>0.314296</td>\n",
       "      <td>-0.492873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.230411</td>\n",
       "      <td>0.027079</td>\n",
       "      <td>-0.626868</td>\n",
       "      <td>-0.047533</td>\n",
       "      <td>-0.005842</td>\n",
       "      <td>-0.069105</td>\n",
       "      <td>0.388540</td>\n",
       "      <td>0.136527</td>\n",
       "      <td>-0.066034</td>\n",
       "      <td>-0.241561</td>\n",
       "      <td>-0.013662</td>\n",
       "      <td>-0.276533</td>\n",
       "      <td>0.487003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.080085</td>\n",
       "      <td>-0.384358</td>\n",
       "      <td>-0.160652</td>\n",
       "      <td>0.243279</td>\n",
       "      <td>0.667355</td>\n",
       "      <td>0.269757</td>\n",
       "      <td>-0.283564</td>\n",
       "      <td>0.035819</td>\n",
       "      <td>-0.237759</td>\n",
       "      <td>0.201738</td>\n",
       "      <td>0.079236</td>\n",
       "      <td>-0.225976</td>\n",
       "      <td>0.080425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.402269</td>\n",
       "      <td>-0.087816</td>\n",
       "      <td>-0.137019</td>\n",
       "      <td>-0.148102</td>\n",
       "      <td>-0.217031</td>\n",
       "      <td>0.083691</td>\n",
       "      <td>0.013088</td>\n",
       "      <td>-0.477169</td>\n",
       "      <td>-0.006349</td>\n",
       "      <td>0.478329</td>\n",
       "      <td>-0.354914</td>\n",
       "      <td>0.044368</td>\n",
       "      <td>0.388613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.432027</td>\n",
       "      <td>-0.027137</td>\n",
       "      <td>-0.127368</td>\n",
       "      <td>-0.115295</td>\n",
       "      <td>-0.102129</td>\n",
       "      <td>0.012096</td>\n",
       "      <td>0.045346</td>\n",
       "      <td>0.812506</td>\n",
       "      <td>-0.046297</td>\n",
       "      <td>0.252342</td>\n",
       "      <td>-0.068729</td>\n",
       "      <td>0.199377</td>\n",
       "      <td>-0.002585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.293968</td>\n",
       "      <td>-0.010532</td>\n",
       "      <td>-0.098881</td>\n",
       "      <td>0.266502</td>\n",
       "      <td>-0.596029</td>\n",
       "      <td>0.397558</td>\n",
       "      <td>-0.422627</td>\n",
       "      <td>0.142698</td>\n",
       "      <td>0.023466</td>\n",
       "      <td>0.179232</td>\n",
       "      <td>0.105490</td>\n",
       "      <td>-0.272754</td>\n",
       "      <td>0.046841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.307128</td>\n",
       "      <td>-0.075415</td>\n",
       "      <td>-0.182650</td>\n",
       "      <td>-0.416587</td>\n",
       "      <td>-0.063870</td>\n",
       "      <td>0.590581</td>\n",
       "      <td>-0.019477</td>\n",
       "      <td>-0.105234</td>\n",
       "      <td>-0.026084</td>\n",
       "      <td>-0.484096</td>\n",
       "      <td>-0.050105</td>\n",
       "      <td>-0.083022</td>\n",
       "      <td>-0.290391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.142639</td>\n",
       "      <td>-0.482877</td>\n",
       "      <td>0.158717</td>\n",
       "      <td>-0.063338</td>\n",
       "      <td>-0.131994</td>\n",
       "      <td>0.274933</td>\n",
       "      <td>0.477130</td>\n",
       "      <td>-0.052833</td>\n",
       "      <td>-0.014899</td>\n",
       "      <td>0.164727</td>\n",
       "      <td>0.518372</td>\n",
       "      <td>0.298570</td>\n",
       "      <td>0.092824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.321261</td>\n",
       "      <td>0.254730</td>\n",
       "      <td>-0.063031</td>\n",
       "      <td>0.402625</td>\n",
       "      <td>-0.116019</td>\n",
       "      <td>0.012558</td>\n",
       "      <td>-0.137230</td>\n",
       "      <td>-0.120765</td>\n",
       "      <td>-0.471394</td>\n",
       "      <td>-0.316026</td>\n",
       "      <td>0.290881</td>\n",
       "      <td>0.388262</td>\n",
       "      <td>0.247681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.396111</td>\n",
       "      <td>0.111578</td>\n",
       "      <td>-0.150712</td>\n",
       "      <td>-0.117104</td>\n",
       "      <td>-0.062839</td>\n",
       "      <td>-0.296013</td>\n",
       "      <td>-0.044112</td>\n",
       "      <td>-0.121072</td>\n",
       "      <td>0.171610</td>\n",
       "      <td>0.146265</td>\n",
       "      <td>0.650355</td>\n",
       "      <td>-0.440757</td>\n",
       "      <td>-0.134054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.241645</td>\n",
       "      <td>-0.404063</td>\n",
       "      <td>0.174421</td>\n",
       "      <td>0.202467</td>\n",
       "      <td>-0.043761</td>\n",
       "      <td>-0.103326</td>\n",
       "      <td>-0.189226</td>\n",
       "      <td>0.077867</td>\n",
       "      <td>0.588480</td>\n",
       "      <td>-0.406250</td>\n",
       "      <td>-0.035434</td>\n",
       "      <td>0.002912</td>\n",
       "      <td>0.373969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0  -0.090411 -0.476735  0.232385 -0.006533 -0.294327 -0.343637  0.065967   \n",
       "1   0.252988 -0.176995 -0.140683 -0.591582  0.001427 -0.252437 -0.546378   \n",
       "2   0.016027 -0.323900 -0.589064  0.296520 -0.119851 -0.229368 -0.016998   \n",
       "3   0.230411  0.027079 -0.626868 -0.047533 -0.005842 -0.069105  0.388540   \n",
       "4  -0.080085 -0.384358 -0.160652  0.243279  0.667355  0.269757 -0.283564   \n",
       "5  -0.402269 -0.087816 -0.137019 -0.148102 -0.217031  0.083691  0.013088   \n",
       "6  -0.432027 -0.027137 -0.127368 -0.115295 -0.102129  0.012096  0.045346   \n",
       "7   0.293968 -0.010532 -0.098881  0.266502 -0.596029  0.397558 -0.422627   \n",
       "8  -0.307128 -0.075415 -0.182650 -0.416587 -0.063870  0.590581 -0.019477   \n",
       "9   0.142639 -0.482877  0.158717 -0.063338 -0.131994  0.274933  0.477130   \n",
       "10 -0.321261  0.254730 -0.063031  0.402625 -0.116019  0.012558 -0.137230   \n",
       "11 -0.396111  0.111578 -0.150712 -0.117104 -0.062839 -0.296013 -0.044112   \n",
       "12 -0.241645 -0.404063  0.174421  0.202467 -0.043761 -0.103326 -0.189226   \n",
       "\n",
       "          7         8         9         10        11        12  \n",
       "0   0.028118 -0.548782 -0.164379 -0.179305 -0.358245 -0.109431  \n",
       "1   0.002873 -0.127716 -0.033814  0.175765  0.296933  0.190071  \n",
       "2  -0.147944  0.141526  0.019954 -0.092880  0.314296 -0.492873  \n",
       "3   0.136527 -0.066034 -0.241561 -0.013662 -0.276533  0.487003  \n",
       "4   0.035819 -0.237759  0.201738  0.079236 -0.225976  0.080425  \n",
       "5  -0.477169 -0.006349  0.478329 -0.354914  0.044368  0.388613  \n",
       "6   0.812506 -0.046297  0.252342 -0.068729  0.199377 -0.002585  \n",
       "7   0.142698  0.023466  0.179232  0.105490 -0.272754  0.046841  \n",
       "8  -0.105234 -0.026084 -0.484096 -0.050105 -0.083022 -0.290391  \n",
       "9  -0.052833 -0.014899  0.164727  0.518372  0.298570  0.092824  \n",
       "10 -0.120765 -0.471394 -0.316026  0.290881  0.388262  0.247681  \n",
       "11 -0.121072  0.171610  0.146265  0.650355 -0.440757 -0.134054  \n",
       "12  0.077867  0.588480 -0.406250 -0.035434  0.002912  0.373969  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print Eigen Vectors, this represent the direction\n",
    "print('\\nEigen Vector\\n')\n",
    "pd.DataFrame(eigen_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\tSelect k eigenvectors that correspond to the k largest eigenvalues,  where k is the dimensionality of the new feature subspace ( kâ‰¤d )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_pairs = []\n",
    "\n",
    "for i in range(len(eigen_vals)):\n",
    "    eigen_pairs.append([np.abs(eigen_vals[i]), eigen_vecs[:,i]])\n",
    "\n",
    "eigen_pairs.sort(reverse=True) #The sorting will help to get the best eigen value at the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the k value:1\n",
      "\n",
      "Eigen Value is : \n",
      " 4.609046238846975\n",
      "\n",
      "and Corresponding Eigen Vector is: \n",
      " [-0.09041052  0.25298779  0.01602653  0.23041073 -0.08008494 -0.40226932\n",
      " -0.43202681  0.29396827 -0.30712828  0.14263937 -0.32126087 -0.39611073\n",
      " -0.24164525]\n"
     ]
    }
   ],
   "source": [
    "k = input('Enter the k value:')\n",
    "k = int(k)\n",
    "\n",
    "if k <= 13:\n",
    "    temp = k-1\n",
    "    print('\\nEigen Value is : \\n', eigen_pairs[temp][0])\n",
    "    print('\\nand Corresponding Eigen Vector is: \\n', eigen_pairs[temp][1])\n",
    "else:\n",
    "    print('The value entered is greater than number of features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\tConstruct a projection matrix W from the \"top\" k eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Before calculating the projection matrix W, lets see how much of the variance can be explained by less components\n",
    "#For that we need to calculate explained variance ratio and cumulative variance ratio\n",
    "\n",
    "#Sum of all eigen values\n",
    "total = sum(eigen_vals)\n",
    "\n",
    "#Sort eigen values in decending order to get the best value at top\n",
    "eigen_vals = list(eigen_vals)\n",
    "eigen_vals.sort(reverse=True)\n",
    "\n",
    "#Explained variance ratio\n",
    "#var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
    "\n",
    "var_exp = []\n",
    "for i in eigen_vals:\n",
    "    var_exp.append(i/total)\n",
    "\n",
    "#Cumulative Explained Variance Ratio\n",
    "cum_var_exp = np.cumsum(var_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now Lets plot them on the graph against the number of components to get the picture of what number of columns explains how much \n",
    "#variance\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    "sns.barplot(x= np.arange(1,14), y=var_exp,color='g', label='Individual Explained Ratio')\n",
    "sns.lineplot(x= np.arange(1,14), y=cum_var_exp, drawstyle='steps-pre', label= 'Cumulative Variance Ratio')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of pricipal components: 4\n",
      "\n",
      "The  4  components explaines  80.08 % variance in the dataset\n"
     ]
    }
   ],
   "source": [
    "k = input('Enter the number of pricipal components: ')\n",
    "k = int(k)\n",
    "\n",
    "print('\\nThe ',k,' components explaines ', (round(cum_var_exp[k],4)*100), '% variance in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top four eigen values explains the 80.08% of the variance in the model, so we will use 4 principal components in the modelling. Now we will create projection matrix W for top 4 eigen values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W Matrix is : \n",
      " [[-0.09041052 -0.47673487  0.23238477 -0.00653254]\n",
      " [ 0.25298779 -0.17699549 -0.14068315 -0.5915821 ]\n",
      " [ 0.01602653 -0.3238999  -0.58906416  0.29652032]\n",
      " [ 0.23041073  0.02707893 -0.6268684  -0.04753301]\n",
      " [-0.08008494 -0.38435766 -0.16065207  0.24327871]\n",
      " [-0.40226932 -0.0878164  -0.1370189  -0.14810192]\n",
      " [-0.43202681 -0.02713712 -0.12736772 -0.11529495]\n",
      " [ 0.29396827 -0.01053185 -0.09888055  0.26650234]\n",
      " [-0.30712828 -0.07541547 -0.18264994 -0.4165868 ]\n",
      " [ 0.14263937 -0.48287748  0.15871686 -0.06333815]\n",
      " [-0.32126087  0.2547302  -0.06303092  0.4026246 ]\n",
      " [-0.39611073  0.11157837 -0.15071199 -0.11710444]\n",
      " [-0.24164525 -0.40406288  0.17442128  0.20246685]]\n"
     ]
    }
   ],
   "source": [
    "#eigen pairs has already been sorted so top 4 eigen vectors are for the best 4 eigen values\n",
    "w= np.array(pd.DataFrame({0:eigen_pairs[0][1],1:eigen_pairs[1][1],2:eigen_pairs[2][1],3:eigen_pairs[3][1]}))\n",
    "\n",
    "print('W Matrix is : \\n', w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.\tTransform the d-dimensional input dataset x using the projection matrix W to obtain the new k-dimensional feature subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to create principal component, it is just the matter of dot multiplication of standardized dataset with projection matrix w\n",
    "\n",
    "x_train_pca = x_train_std.dot(w)\n",
    "x_test_pca = x_test_std.dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.957490</td>\n",
       "      <td>2.175798</td>\n",
       "      <td>0.093707</td>\n",
       "      <td>0.609160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.550131</td>\n",
       "      <td>-1.090312</td>\n",
       "      <td>-1.018142</td>\n",
       "      <td>0.591996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.322133</td>\n",
       "      <td>-1.142327</td>\n",
       "      <td>-1.179062</td>\n",
       "      <td>0.990723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.396600</td>\n",
       "      <td>1.400550</td>\n",
       "      <td>-1.756073</td>\n",
       "      <td>-3.500345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.889224</td>\n",
       "      <td>-0.109286</td>\n",
       "      <td>1.183957</td>\n",
       "      <td>0.770093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3\n",
       "0  0.957490  2.175798  0.093707  0.609160\n",
       "1 -2.550131 -1.090312 -1.018142  0.591996\n",
       "2 -2.322133 -1.142327 -1.179062  0.990723\n",
       "3 -1.396600  1.400550 -1.756073 -3.500345\n",
       "4  2.889224 -0.109286  1.183957  0.770093"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x_train_pca).head() #This is the training dataset for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_pca.shape #Shape of new train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.141924</td>\n",
       "      <td>-1.095473</td>\n",
       "      <td>0.145489</td>\n",
       "      <td>1.093364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.204226</td>\n",
       "      <td>1.700479</td>\n",
       "      <td>-0.861102</td>\n",
       "      <td>0.236013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.202638</td>\n",
       "      <td>-1.474316</td>\n",
       "      <td>0.169427</td>\n",
       "      <td>-0.514937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.099270</td>\n",
       "      <td>0.635164</td>\n",
       "      <td>0.895397</td>\n",
       "      <td>0.498790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.254636</td>\n",
       "      <td>-2.399471</td>\n",
       "      <td>-0.045967</td>\n",
       "      <td>1.299334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3\n",
       "0  2.141924 -1.095473  0.145489  1.093364\n",
       "1 -0.204226  1.700479 -0.861102  0.236013\n",
       "2 -2.202638 -1.474316  0.169427 -0.514937\n",
       "3 -2.099270  0.635164  0.895397  0.498790\n",
       "4 -2.254636 -2.399471 -0.045967  1.299334"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x_test_pca).head() #This is the test dataset for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_pca.shape #Shape of new test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Model using these PCAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for PCA components using Linear Model is : 0.9630\n"
     ]
    }
   ],
   "source": [
    "#Take an example of Logistic regression model\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_train_pca, y_train)\n",
    "ypred = logreg.predict(x_test_pca)\n",
    "logacc = metrics.accuracy_score(y_test, ypred)\n",
    "\n",
    "print('The accuracy for PCA components using Linear Model is : %1.4f' %(logacc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23,  0,  0],\n",
       "       [ 2, 17,  0],\n",
       "       [ 0,  0, 12]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The confusion matrix\n",
    "\n",
    "metrics.confusion_matrix(y_test,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      1.00      0.96        23\n",
      "           2       1.00      0.89      0.94        19\n",
      "           3       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           0.96        54\n",
      "   macro avg       0.97      0.96      0.97        54\n",
      "weighted avg       0.97      0.96      0.96        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification Report\n",
    "\n",
    "print(metrics.classification_report(y_test,ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see by using only 4 columns instead of 13, we are able to get 96% accuracy score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
